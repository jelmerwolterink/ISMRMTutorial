{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content description\n",
    "This is a Jupyter notebook that closely follows the MONAI tutorials provided in the [MONAI repository](https://github.com/Project-MONAI/tutorials). The notebook is specifically tuned towards segmentation of data in the [ACDC challenge](https://www.creatis.insa-lyon.fr/Challenge/acdc/) using a 2D U-Net. You will need to first download the ACDC dataset via [this link](https://acdc.creatis.insa-lyon.fr/#challenges), and unzip it in a directory of your choosing. You can define that directory here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = r'/home/wolterinkjm/ACDC/training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing ACDC data\n",
    "\n",
    "We will first show how to load and visualize ACDC images and segmentations. First, import the Python packages used in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install monai nibabel\n",
    "import torch\n",
    "import monai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a [MONAI dictionary transform](https://docs.monai.io/en/latest/transforms.html#dictionary-transforms) that loads a pair of image + reference label based on filenames provided in a Python dictionary. The `LoadImageD` function does not yet load the data, but provides the object that will be able to load the data given new filenames. Afterwards, we provide a dictionary containing one pair of image + reference label filename to this object, and load the image and label into 1data_dict1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = monai.transforms.LoadImageD((\"image\", \"label\")) \n",
    "file_dict = {\"image\": \"{}/patient001/patient001_frame01.nii.gz\".format(datapath), \n",
    "             \"label\": \"{}/patient001/patient001_frame01_gt.nii.gz\".format(datapath)}\n",
    "\n",
    "data_dict = transform(file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `data_dict` contains a 3D cine MR image and its reference segmentation mask. We use the [Matplotlib library](https://matplotlib.org/) to visualize the images in a rectangular grid. The for-loop iterates over `z`, the slice index. Each slice is shown, and the reference label is shown as a colored overlay (left ventricle in green, myocardium in blue, right ventricle in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(pt_dict, batch=False):\n",
    "    image = pt_dict[\"image\"].squeeze()\n",
    "    label = pt_dict[\"label\"].squeeze()\n",
    "    if batch:\n",
    "        image = image.permute((1, 2, 0))\n",
    "        label = label.permute((1, 2, 0))\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for z in range(image.shape[2]): \n",
    "      plt.subplot(np.ceil(np.sqrt(image.shape[2])), np.ceil(np.sqrt(image.shape[2])), 1 + z)\n",
    "      plt.imshow(image[:, :, z], cmap='gray')\n",
    "      plt.axis('off')\n",
    "      plt.imshow(np.ma.masked_where(label[:, :, z]!=2, label[:, :, z]==2), alpha=0.6, cmap='Blues', clim=(0, 1))  \n",
    "      plt.imshow(np.ma.masked_where(label[:, :, z]!=3, label[:, :, z]==3), alpha=0.6, cmap='Greens', clim=(0, 1))\n",
    "      plt.imshow(np.ma.masked_where(label[:, :, z]!=1, label[:, :, z]==1), alpha=0.6, cmap='Reds', clim=(0, 1))\n",
    "      plt.title('Slice {}'.format(z + 1))\n",
    "    plt.show()\n",
    "    \n",
    "visualize_data(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing transforms\n",
    "Multiple PyTorch/MONAI transforms can be composed in to one transform to pre-process or post-process data. For example, we can load an image + reference label pair, normalize the image, and add Gaussian noise to the image. To do this, we add a `RandGaussianNoised` transform and apply this only to `image`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = monai.transforms.Compose([monai.transforms.LoadImageD((\"image\", \"label\")), \n",
    "                                      monai.transforms.ScaleIntensityRangePercentilesd(keys=(\"image\"), lower=5, upper=95, b_min=0, b_max=1, clip=True),\n",
    "                                      monai.transforms.RandGaussianNoised((\"image\"), prob=1, std=.3)])\n",
    "\n",
    "data_dict = transform(file_dict)\n",
    "visualize_data(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and dataloaders\n",
    "We can also load more than one image, and instead store a full set of images and labels in a dataset. To do this, we make a dictionary of filenames, as before. This dictionary and the composed transform, are then used to initialize a dataset. In addition to the `LoadImageD` transform, we now also resample the image to a fixed resolution using `SpacingD`, add a channel using `AddChannelD`, normalize the intensities using `ScaleIntensityRangePercentilesd`, and randomly crop a 128 x 128 pixel subimage using `RandSpatialCropD`, and finally transform the NumPy arrays for the image and reference label mask into PyTorch tensors using `ToTensorD`. \n",
    "\n",
    "This [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) provides more information about datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = []\n",
    "for ptid in range(1, 101):\n",
    "    gt_filenames = glob.glob(r'{}/patient{}/*_gt.nii.gz'.format(datapath, str(ptid).zfill(3)))\n",
    "    file_dict.append({'image': gt_filenames[0].replace('_gt', ''), 'label': gt_filenames[0]})\n",
    "    file_dict.append({'image': gt_filenames[1].replace('_gt', ''), 'label': gt_filenames[1]})\n",
    "    \n",
    "transform = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImageD((\"image\", \"label\")),\n",
    "    monai.transforms.AddChannelD((\"image\", \"label\")),\n",
    "    monai.transforms.ScaleIntensityRangePercentilesd(keys=(\"image\"), lower=5, upper=95, b_min=0, b_max=1, clip=True),\n",
    "    monai.transforms.RandSpatialCropD(keys=(\"image\", \"label\"), roi_size=(128, 128, 1), random_center=True, random_size=False),\n",
    "    monai.transforms.SqueezeDimd(keys=(\"image\", \"label\"), dim=-1),\n",
    "    monai.transforms.ToTensorD((\"image\", \"label\"))        \n",
    "])    \n",
    "    \n",
    "dataset = monai.data.Dataset(data = file_dict, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we make a dataloader for the dataset. This is an object that allows us to efficiently sample data from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = monai.data.DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect this dataset and dataloader. For example, we can print how many images the dataset contains. We can also draw and visualize random mini-batches of samples using the dataloader. Running the cell below a couple of times will give different samples each time. Note that these samples are subimages with fixed size, as we'd like to use to train the U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset contains {} images'.format(len(dataset)))\n",
    "\n",
    "visualize_data(next(iter(dataloader)), batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a training and validation set\n",
    "When training the U-Net for segmentation, it is important to separate your training and validation sets well. Using data from the same patient in both the training and validation set could lead to [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)). Hence, we split our dataset at the patient level: we select 80 patients for training, and 20 patients for testing. Because ACDC patients are organized in groups of 20 patients with the same pathology, we perform a stratified selection in which we put each fifth patient in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = set(range(1, 101, 5))\n",
    "train_ids = set(range(1, 101)) - val_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make two separate file dictionaries, one for the training set and one for the validation set. These will be used for separate datasets and separate dataloaders. Note that we also define a separate transform for the training set and validation set. When using data augmentation, the transform for the training set should be altered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict_train = []\n",
    "for ptid in train_ids:\n",
    "    gt_filenames = glob.glob(r'{}/patient{}/*_gt.nii.gz'.format(datapath, str(ptid).zfill(3)))\n",
    "    file_dict_train.append({'image': gt_filenames[0].replace('_gt', ''), 'label': gt_filenames[0]})\n",
    "    file_dict_train.append({'image': gt_filenames[1].replace('_gt', ''), 'label': gt_filenames[1]})\n",
    "    \n",
    "file_dict_val = []\n",
    "for ptid in val_ids:\n",
    "    gt_filenames = glob.glob(r'{}/patient{}/*_gt.nii.gz'.format(datapath, str(ptid).zfill(3)))\n",
    "    file_dict_val.append({'image': gt_filenames[0].replace('_gt', ''), 'label': gt_filenames[0]})\n",
    "    file_dict_val.append({'image': gt_filenames[1].replace('_gt', ''), 'label': gt_filenames[1]})    \n",
    "       \n",
    "# This transform should be altered to add data augmentation        \n",
    "transform_train = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImageD((\"image\", \"label\")),\n",
    "    monai.transforms.AddChannelD((\"image\", \"label\")),\n",
    "    monai.transforms.ScaleIntensityRangePercentilesd(keys=(\"image\"), lower=5, upper=95, b_min=0, b_max=1, clip=True),\n",
    "    monai.transforms.RandSpatialCropD(keys=(\"image\", \"label\"), roi_size=(128, 128, 1), random_center=True, random_size=False),\n",
    "    monai.transforms.SqueezeDimd(keys=(\"image\", \"label\"), dim=-1),    \n",
    "    monai.transforms.ToTensorD((\"image\", \"label\")),\n",
    "])\n",
    "\n",
    "transform_val = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImageD((\"image\", \"label\")),\n",
    "    monai.transforms.AddChannelD((\"image\", \"label\")),\n",
    "    monai.transforms.ScaleIntensityRangePercentilesd(keys=(\"image\"), lower=5, upper=95, b_min=0, b_max=1, clip=True),\n",
    "    monai.transforms.ToTensorD((\"image\", \"label\")),\n",
    "])\n",
    "        \n",
    "dataset_train = monai.data.CacheDataset(data = file_dict_train, transform = transform_train, progress=False)\n",
    "dataset_val = monai.data.Dataset(data = file_dict_val, transform = transform_val)\n",
    "\n",
    "dataloader_train = monai.data.DataLoader(dataset_train, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dataloader_val = monai.data.DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "\n",
    "print('The training set contains {} MRI scans.'.format(len(dataset_train)))\n",
    "print('The test set contains {} MRI scans.'.format(len(dataset_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize a mini-batch from the training set, which contains randomly selected 2D 128 x 128 pixel images. Similarly, we can visualize all 2D slices for one 3D volume in the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(next(iter(dataloader_train)), batch=True)\n",
    "visualize_data(next(iter(dataloader_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the neural network, loss function, and optimizer\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "With data loading all sorted out, we set up the U-Net, a loss function, and an optimizer. If possible, use a GPU to substantially speed up computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a [U-Net](https://arxiv.org/abs/1505.04597) for segmentation in MONAI is as easy as calling the `UNet` function and providing it with the number of input channels, output channels, and feature maps/channels in the intermediate layers. The following provides us with a model that is optimized during training to perform segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = monai.networks.nets.UNet(\n",
    "    dimensions=2,\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function should reflect what we want from the trained model. In this case, a [Dice](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) is used, which becomes lower as automatic and reference segmentations overlap more. This loss function first applies a [softmax](https://en.wikipedia.org/wiki/Softmax_function) function to the network outputs so that the probabilities for all classes sum to one. To make sure that the predicted probability mask `y_pred` and the reference segmentation mask `y` have the same shape, a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) is applied to the reference segmentation mask. The Dice loss is computed over the full mini-batch (`batch=True`) to avoid poorly defined loss in individual batch samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function =  monai.losses.DiceLoss(softmax=True, to_onehot_y=True, batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer algorithm is chosen that performs gradient descent on the network parameters to minimize the loss function. In many cases, [Adam](https://arxiv.org/abs/1412.6980) is a good default option. The optimizer operates on the parameters of the previously defined U-Net, i.e. `model`. A learning rate `lr` is provided to the optimizer, which defines how large the changes should be that are made to the network parameters in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a simple training loop\n",
    "The below provides a minimal training loop. The loop repeatedly iterates over the training set. Each such full training set iteration is called an epoch, and we perform 500 such epochs. Within an epoch, the loop does the following\n",
    "* Pick a random mini-batch from the training set\n",
    "* Set the gradients of all parameters in the network using `optimizer.zero_grad()`\n",
    "* Obtain model output for the input batch \n",
    "* Compute the Dice loss\n",
    "* Backpropagate the loss using `loss.backward()`\n",
    "* Let the optimizer update the network parameters with `optimizer.step()`\n",
    "* Pick the next random mini-batch\n",
    "\n",
    "This should take around 5 minutes on a reasonable GPU or 30 minutes on a decent CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "training_losses = list()\n",
    "\n",
    "for epoch in tqdm(range(500)):\n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in dataloader_train: \n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data[\"image\"].to(device))\n",
    "        loss = loss_function(outputs, batch_data[\"label\"].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    training_losses.append(epoch_loss/step)\n",
    "\n",
    "# Store the network parameters        \n",
    "torch.save(model.state_dict(), r'trainedUNet.pt')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can plot the training loss, with number of epochs on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.asarray(training_losses))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice loss')\n",
    "plt.show()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to segment the full validation set, where some post-processing is applied to each segmentation to obtain a contiguous segmentation mask with discrete values. The following loop also prints the average Dice similarity coefficient for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "postprocess = monai.transforms.Compose([\n",
    "    monai.transforms.AsDiscrete(argmax=True, to_onehot=True, n_classes=4, threshold_values=False),\n",
    "    monai.transforms.KeepLargestConnectedComponent(applied_labels=(1, 2, 3), independent=False, connectivity=None)\n",
    "])\n",
    "\n",
    "for val_batch in dataloader_val:\n",
    "    outputs_val = monai.inferers.sliding_window_inference(val_batch[\"image\"].squeeze(1).permute(3, 0, 1, 2).to(device), (128, 128), 32, model, overlap = 0.8)\n",
    "    outputs_val = outputs_val.permute(1, 2, 3, 0).unsqueeze(0)\n",
    "    print(outputs_val.shape)\n",
    "    outputs_val = postprocess(outputs_val)\n",
    "    result = {\"image\": val_batch[\"image\"].squeeze(), \n",
    "              \"label\": torch.argmax(outputs_val, dim=1).squeeze().cpu()}\n",
    "    visualize_data(result)     \n",
    "    \n",
    "    dice_metric = monai.metrics.DiceMetric()\n",
    "    dsc, _ = dice_metric(outputs_val.cpu(), monai.networks.utils.one_hot(val_batch[\"label\"].squeeze().unsqueeze(0).unsqueeze(0), 4))\n",
    "    hd_metric = monai.metrics.HausdorffDistanceMetric()\n",
    "    hd, _ = hd_metric(outputs_val.cpu(), monai.networks.utils.one_hot(val_batch[\"label\"].squeeze().unsqueeze(0).unsqueeze(0), 4))\n",
    "    \n",
    "    print('Average DSC {:.2f}, average Hausdorff distance {:.2f} mm'.format(dsc[0], hd[0]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can also load pretrained network parameters. For that, run the following after defining your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r'trainedUNet.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
